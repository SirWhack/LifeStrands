{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "WebFetch(domain:docs.anthropic.com)",
      "Bash(make:*)",
      "Bash(sudo systemctl status:*)",
      "Bash(docker:*)",
      "Bash(getent:*)",
      "Bash(claude doctor)",
      "Bash(claude --version)",
      "Bash(rmdir:*)",
      "Bash(echo:*)",
      "Bash(curl:*)",
      "Bash(grep:*)",
      "Bash(pip show:*)",
      "Bash(lspci:*)",
      "Bash(apt list:*)",
      "Bash(./check_rocm.bat)",
      "Bash(python test:*)",
      "Bash(python3:*)",
      "Bash(ss:*)",
      "Bash(ip route:*)",
      "Bash(timeout:*)",
      "Bash(pip uninstall:*)",
      "Bash(pip --version)",
      "Bash(pip3:*)",
      "Bash(source:*)",
      "Bash(rocm_env/Scripts/python -m pip list:*)",
      "Bash(./rocm_env/Scripts/activate)",
      "Bash(./rocm_env/Scripts/python.exe -m pip list)",
      "Bash(./rocm_env/Scripts/python.exe -m pip install llama-cpp-python --force-reinstall --no-cache-dir --verbose)",
      "Bash(CMAKE_ARGS=\"-DGGML_VULKAN=ON\" ./rocm_env/Scripts/python.exe -m pip install llama-cpp-python --force-reinstall --no-cache-dir --verbose)",
      "Bash(vulkaninfo:*)",
      "Bash(./rocm_env/Scripts/python.exe -m pip install llama-cpp-python --force-reinstall --no-cache-dir)",
      "Bash(./rocm_env/Scripts/python.exe test_gpu.py)",
      "Bash(where vulkaninfo)",
      "WebFetch(domain:www.lunarg.com)",
      "Bash(./check_vulkan.bat)",
      "Bash(./rocm_env/Scripts/python.exe -m pip uninstall llama-cpp-python -y)",
      "Bash(./rocm_env/Scripts/python.exe test_gpu_load.py)",
      "Bash(mv:*)",
      "Bash(chmod:*)",
      "Bash(powershell.exe:*)",
      "Bash(git pull:*)",
      "Bash(git config:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(git push:*)",
      "Bash(ENABLE_EMBEDDINGS=false docker-compose up -d npc-service)",
      "Bash(ENABLE_EMBEDDINGS=false docker-compose up -d)",
      "Bash(npm install)",
      "Bash(npm run dev:*)",
      "Bash(python:*)",
      "Bash(cd:*)",
      "Bash(scripts/start_model_service_wsl.sh:*)",
      "Bash(cat:*)",
      "Bash(export MODELS_PATH=\"D:\\AI\\Life Strands v2\\Models\")",
      "Bash(set LM_STUDIO_MODE=true)",
      "Bash(set MOCK_MODE=false)",
      "Bash(pip install:*)",
      "Bash(rm:*)",
      "mcp__playwright__browser_navigate",
      "mcp__playwright__browser_snapshot",
      "mcp__playwright__browser_take_screenshot",
      "mcp__playwright__browser_console_messages",
      "mcp__playwright__browser_network_requests",
      "mcp__playwright__browser_click",
      "mcp__playwright__browser_wait_for",
      "mcp__sequential-thinking__sequentialthinking",
      "mcp__playwright__browser_evaluate",
      "WebSearch",
      "Bash(npm run type-check:*)",
      "Bash(npm run lint)",
      "Bash(npm run build:*)",
      "mcp__playwright__browser_type",
      "mcp__playwright__browser_press_key",
      "Bash(.dev.ps1 status)",
      "Bash(.dev.ps1 health)",
      "Bash(dev.bat status)",
      "Bash(copy main.py main_simple_backup.py)",
      "Bash(copy main_broken.py main.py)",
      "Bash(cp:*)",
      "Bash(tasklist:*)",
      "WebFetch(domain:huggingface.co)",
      "WebFetch(domain:github.com)",
      "Bash(git submodule:*)",
      "Bash(cmake:*)",
      "Bash(set CMAKE_ARGS=\"-DGGML_VULKAN=1\")",
      "Bash(set FORCE_CMAKE=1)",
      "Bash(dir:*)",
      "Bash(find:*)",
      "Bash(./scripts/start_native_services.sh:*)",
      "Bash(./scripts/test_hybrid_deployment.sh:*)",
      "Bash(pkill:*)",
      "Bash(./start_native.sh:*)",
      "Bash(scripts/start_native_services.sh:*)",
      "Bash(scripts/stop_native_services.sh:*)",
      "Bash(kill:*)"
    ],
    "deny": []
  },
  "outputStyle": "default"
}